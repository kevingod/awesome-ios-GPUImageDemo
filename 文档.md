##基本概念
>1，`GPUImageFilter`：
>>`GPUImageFilter`继承自`GPUImageOutPut`,同时遵循了`<GPUImageInput>`协议，可以看作`GPUImage`响应链中的一环，在响应链中提供纹理(基于OpenGLES)处理图片。每个`filter`都可以有多个分支，即`filter`可以`add`多个`target`，每一条新的响应链都必须具有完整的头尾，即以`output`开始，以`input`收尾。


>2，业务层输出对象（提供源），四大金刚
>
>>2.1 `GPUImageVideoCamera`，拍摄对象。
>>
>>2.2 `GPUImagePicture` 图片对象。
>>
>>2.3 `GPUImageMovie` 视频文件（流）对象。
>>
>>2.4 `GPUImageStillCamera`照相（感觉这个有点多余，GPUImageVideoCamera去音频版）


>3，业务层输入对象（接收结果）
>>
>>3.1 `GPUImageView`，接收纹理并展示
>>
>>3.2 `GPUImageMovieWriter` ，接收buffer并写入文件，输入端（`GPUImageVideoCamera`，`GPUImageMovie`）需要设置音频轨道的`target`到此对象，而不是通过响应链传过来。即`input.audioEncodingTarget = GPUImageMovieWriter`


最容易搞反的就是，遵循input协议的对象是用来接收的，继承自output的对象是用来输出的。
##内部实现解析
###openGL部分简单解析（可略）
>1，获取纹理坐标
>>`+ (const GLfloat *)textureCoordinatesForRotation:(GPUImageRotationMode)rotationMode;`
>
>2，绘制结果输出
>> 绘制的结果后输入到outputframebuffer指定的缓存(`usingNextFrameForImageCapture`代表着输出的结果会被用于获取图像，所以在绘制之前要加锁`[outputFramebuffer lock]`)
>
>3，绑定输入纹理
>>`glBindTexture(GL_TEXTURE_2D, [firstInputFramebuffer texture]);`
>
>4，绑定顶点和纹理坐标并绘制图元
>>` glVertexAttribPointer(filterPositionAttribute, 2, GL_FLOAT, 0, 0, vertices);`
>>`glVertexAttribPointer(filterTextureCoordinateAttribute, 2, GL_FLOAT, 0, 0, textureCoordinates);`
>>`glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);`
>
>5，纹理解锁
>>`[firstInputFramebuffer unlock];` 输入纹理使用完毕，解锁。在调用这个解锁之前必须确定之前已经调用加锁，否则会报错。
`GPUImageFramebuffer`使用引用计数来管理缓存，当引用计数小于0的时候会回收缓存。


###业务层简单解析
>1，output类获取到输入源，传给filter
>
>2，filter绘制完，传给响应链下一个filter
>
>3，所有的filter都绘制完成（如果设置了usingNextFrameForImageCapture，则是由GCD信号来通知）
>
>4，输出给input
    

##关于视频继续拍摄的实现逻辑
>其实超简单啦，每一次中断，下一次都是重新拍摄写入文件的。然后拍完了获得的是一组视频文件（AVAsset）。这一组文件可以合成一个AVMutableComposition对象，由于AVMutableComposition继承自AVAsset，那么一切操作都顺理成章。最后写入文件就可以合并成一个文件了。同理，视频头条的视频编辑页面，如果有多个视频片段，实际上也是多个视频文件而已（只是看起来像是一个视频）。
>
>参考`VideoRecordViewController`268行


















##遇到的坑：

>1，视图未显示出来的时候，尝试将图片渲染到视图上会导致大量资源消耗。并且视图加载出来之后不能正常渲染。所以用`GPUImage`的`GPUImageMoview` output到`GPUImageView`上必须是在`viewDidAppear`之后调用`startProcess`
>
>2，渲染到视图之外会占用大量资源。
>
>3，调用了响应链末尾的input的`startProgress`或者`startRecording`却没有开始绘制：因为绘制开始是有输出端决定的，所以应该调用响应链头的start 
>
>4，拍摄视频没有声音,设置GPUImageVideoCamera、GPUImageMovie的`audioEncodingTarget`
>
>5，写的demo超级卡、push到下一个控制器直接卡死，回顾1、2，push之前要停止上一个视图的渲染。
>
>6，GPUImageView闪，视频文件闪。响应链中同时有几个output addTargert到这个input了。


